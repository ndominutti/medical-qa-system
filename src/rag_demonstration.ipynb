{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e87e413",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This notebook will be in charge of demonstrating how the system works. I not intended by any chance that this approach would be used in a productive environment (not even development) as there are other options as dockerizing and hosting the service in a microservice-like architecture (using fastAPI for example) or even recurring to cloud based solutions as AWS Bedrock to create a flow and expose as an endpoint without the overhead of de deployment process.\n",
    "\n",
    "Nevertheles, one could think that the job that this notebook will do is basically what would happen at a user query time. The proposed architecture is only composed of 2 main steps, the retrieval and generation, this is not a productive architecture by any chance as other several components as *intent classifiers*, *routers*, *guardrails* could be applied too (always balancing the cost-latency tradeoff)\n",
    "\n",
    "The reasons on why this approach was selected to address this takehome assesment are already detailed in the README file you can find at the root of this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153f0e6",
   "metadata": {},
   "source": [
    "---\n",
    "# Setup\n",
    "\n",
    "Before we start, it's needed to perform some setup for our environment as installing the needed libraries, loading the ENV variables, config files and downloading the data model that was trained in the training pipeline (please refer to /model_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ee74c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/Users/nicolas.dominutti/Desktop/ml/medical-qa-system/.venv/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/Users/nicolas.dominutti/Desktop/ml/medical-qa-system/.venv/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/Users/nicolas.dominutti/Desktop/ml/medical-qa-system/.venv/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/Users/nicolas.dominutti/Desktop/ml/medical-qa-system/.venv/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/Users/nicolas.dominutti/Desktop/ml/medical-qa-system/.venv/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -andas (/Users/nicolas.dominutti/Desktop/ml/medical-qa-system/.venv/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.3; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/nicolas.dominutti/Desktop/ml/medical-qa-system/.venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet -r rag_system/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73046766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"rag_system/.env\")\n",
    "MODEL_PATH = 'rag_system/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b229e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_utils import load_config\n",
    "CONFIG = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c346cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_utils import S3Manager\n",
    "s3_client = S3Manager.get_client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22577319",
   "metadata": {},
   "source": [
    "Specifically in the following cell I will download the fine-tuned FlagEmbedding bge-base-en-v1.5 model, then I'll load it utilizing the FlagEmbedding library. It's important to recall that the selected model is *instruction tuned*, so I will pass it the very same instruction used at fine-tuning time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b474edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.listdir(MODEL_PATH):\n",
    "    #run only if the model was not already downloaded\n",
    "    S3Manager.download_folder(s3_client, CONFIG['S3_BUCKET'], 'finetuned_model/', MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f90aae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nicolas.dominutti/Desktop/ml/medical-qa-system/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from general_utils.flagembedding import FlagEmbeddingManager\n",
    "embedding_service = FlagEmbeddingManager()\n",
    "model = embedding_service.get_model(\n",
    "    local_model_path=MODEL_PATH,\n",
    "    query_instruction=CONFIG['QUERY_INSTRUCTION_AT_RETRIEVAL']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf28d7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilized instruction: 'Represent this sentence for searching relevant passages:'\n"
     ]
    }
   ],
   "source": [
    "print(f\"Utilized instruction: '{CONFIG['QUERY_INSTRUCTION_AT_RETRIEVAL']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd21e5d6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c355655",
   "metadata": {},
   "source": [
    "# Services access\n",
    "\n",
    "After the setup is done, I'll proceed with initializing the connection to the needed services, as stated in the README file I will use:\n",
    "* AWS OpenSearch: as the vector DB that stores the corpus embeddings produced with my fine-tuned model (please refer to /training) and allows me to perform semantic retrieval on them\n",
    "* AWD Bedrock: as the fully managed solution to call the generator model API\n",
    "\n",
    "The BedrockManager class (can be found in rag_system.utils.bedrock) encapsulates all the needed logic to query Bedrock. While the FlagEmbeddingManager class (can be found in general_utils.flagembedding) encapsulate the logic to connect and search from OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eedc8184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag_system.utils import BedrockManager\n",
    "bedrock = BedrockManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ee57d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea9d1c",
   "metadata": {},
   "source": [
    "# Some examples\n",
    "\n",
    "Here I'll provide some examples of retrieved context and responses from the model. It's worth it to notice that the utilized system prompt was not tuned due to a lack of time, being that a possible improvement spot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba7f1d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilized untuned system prompt:  You are a knowledgeable and empathetic medical assistant. Your task is to answer patients' questions strictly based on the context provided below. Do not include any information outside of this context. If the answer is not contained within the context, respond politely that you donâ€™t have enough information to answer.\n",
      "Context {CONTEXT} \n"
     ]
    }
   ],
   "source": [
    "print(f\"Utilized untuned system prompt: {CONFIG['SYSTEM_PROMPT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27f80733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def chat(query:str) -> Dict[str,Any]:\n",
    "    \"\"\"\n",
    "    Auxiliar function to concatenate the retrieval and generation steps.\n",
    "\n",
    "    Args:\n",
    "        query(str): user's query\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str,Any]: returned dict from Bedrock InvokeModel action. Contains\n",
    "        the generation answer under the key 'generation'\n",
    "    \"\"\"\n",
    "    context = embedding_service.search(\n",
    "        endpoint_url=CONFIG['OPENSEARCH_INDEX_URL'],\n",
    "        flag_embedding_model=model, \n",
    "        query=query,\n",
    "        top_k=CONFIG['ITEMS_TO_RETRIEVE']\n",
    "    )\n",
    "    return bedrock.ask(\n",
    "        query,\n",
    "        context,\n",
    "        CONFIG['MODEL'],\n",
    "        CONFIG['SYSTEM_PROMPT'],\n",
    "        CONFIG['TEMPERATURE'],\n",
    "        CONFIG['TOP_P'],\n",
    "        CONFIG['MAX_OUTPUT_TOKENS']\n",
    "    ), context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2702122d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://search-medical-qa-system-wllp2yik3gws7durfruiomvfky.us-east-2.es.amazonaws.com/embedding-finetuned-v1'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG[\"OPENSEARCH_INDEX_URL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13c64d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = model.encode(\"What is high blood pressure?\").tolist()\n",
    "payload = {\n",
    "            \"query\": {\"knn\": {\"vector_field\": {\"vector\": query_vector, \"k\": CONFIG[\"ITEMS_TO_RETRIEVE\"]}}}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87568fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [502]>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "73dbd4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.post(\n",
    "    url=f'{CONFIG[\"OPENSEARCH_INDEX_URL\"]}/_search',\n",
    "    json=payload,\n",
    "    auth=flag_embedding.awsauth,\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8afd8cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in search with args=(<general_utils.flagembedding.FlagEmbeddingManager object at 0x11a786fe0>,), kwargs={'endpoint_url': 'https://search-medical-qa-system-wllp2yik3gws7durfruiomvfky.us-east-2.es.amazonaws.com/embedding-finetuned-v1', 'flag_embedding_model': <FlagEmbedding.inference.embedder.encoder_only.base.BaseEmbedder object at 0x11a786fb0>, 'query': 'What is high blood pressure?', 'top_k': 3} | Exception: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/ml/medical-qa-system/.venv/lib/python3.10/site-packages/requests/models.py:976\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/json/decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response, context \u001b[38;5;241m=\u001b[39m \u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is high blood pressure?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeneration\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m context]))\n",
      "Cell \u001b[0;32mIn[17], line 14\u001b[0m, in \u001b[0;36mchat\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mchat\u001b[39m(query:\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m,Any]:\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Auxiliar function to concatenate the retrieval and generation steps.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m        the generation answer under the key 'generation'\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43membedding_service\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOPENSEARCH_INDEX_URL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mflag_embedding_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mITEMS_TO_RETRIEVE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bedrock\u001b[38;5;241m.\u001b[39mask(\n\u001b[1;32m     21\u001b[0m         query,\n\u001b[1;32m     22\u001b[0m         context,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m         CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAX_OUTPUT_TOKENS\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     28\u001b[0m     ), context\n",
      "File \u001b[0;32m~/Desktop/ml/medical-qa-system/src/general_utils/logging.py:12\u001b[0m, in \u001b[0;36mlog.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m         logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccess \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Desktop/ml/medical-qa-system/src/general_utils/flagembedding.py:49\u001b[0m, in \u001b[0;36mFlagEmbeddingManager.search\u001b[0;34m(self, endpoint_url, flag_embedding_model, query, top_k)\u001b[0m\n\u001b[1;32m     40\u001b[0m query_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed_query(flag_embedding_model, query)\n\u001b[1;32m     41\u001b[0m payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mknn\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector_field\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvector\u001b[39m\u001b[38;5;124m\"\u001b[39m: query_vector, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_k}}}\n\u001b[1;32m     43\u001b[0m }\n\u001b[1;32m     44\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mendpoint_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/_search\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawsauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mContent-Type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapplication/json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m---> 49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [hit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_source\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m hit \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhits\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m3\u001b[39m]]\n",
      "File \u001b[0;32m~/Desktop/ml/medical-qa-system/.venv/lib/python3.10/site-packages/requests/models.py:980\u001b[0m, in \u001b[0;36mResponse.json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "response, context = chat(\"What is high blood pressure?\")\n",
    "print(response['generation'])\n",
    "print(\"\\n\".join([x for x in context]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72557d7",
   "metadata": {},
   "source": [
    "> _Disclaimer_\n",
    "\n",
    "Latency is a topic that was not addressed at all:\n",
    "* for this test the embedding model was running at full precision (fp32) in a CPU\n",
    "* the machine that hosts the AWS OpenSearch collection is the smallest one can get (t3.small)\n",
    "\n",
    "One could think of hosting the embedding model in a Sagemaker Endpoint or also in a Bedrock one, reducing precision if needed and increase the machine size behind OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc63b75",
   "metadata": {},
   "source": [
    "### Evaluating the generation\n",
    "\n",
    "One could evaluate this generation in different ways:\n",
    "* reference free: for example with an LLM-as-a-judge to analize the helpfulness, toxicity and faithfulness from the answer\n",
    "* reference based: metrics that leverage the fact that there is a golden dataset with labels for some answers\n",
    "\n",
    "\n",
    "In my case, just as a final validation I will utilize the dataset that was provided as part of the assignment, I will follow a simple approach:\n",
    "\n",
    "1. select a sample question\n",
    "2. generate the response to that question\n",
    "3. compare the generated response to the golden answer by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b26d177d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_model = flagg_embedding.get_model(\n",
    "    local_model_path=\"BAAI/bge-large-en-v1.5\",\n",
    "    query_instruction=CONFIG['QUERY_INSTRUCTION_AT_RETRIEVAL'],\n",
    "    use_fp16=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
