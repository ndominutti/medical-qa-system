{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66cd67c8-2e3e-4f74-a5d6-b42d1f8974aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T22:16:32.477713Z",
     "iopub.status.busy": "2025-07-11T22:16:32.477320Z",
     "iopub.status.idle": "2025-07-11T22:16:33.976147Z",
     "shell.execute_reply": "2025-07-11T22:16:33.975493Z",
     "shell.execute_reply.started": "2025-07-11T22:16:32.477697Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q -r model_training/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81222938-1a06-4b75-884d-b44d6e8be2a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T22:23:06.092128Z",
     "iopub.status.busy": "2025-07-11T22:23:06.091966Z",
     "iopub.status.idle": "2025-07-11T22:23:07.740360Z",
     "shell.execute_reply": "2025-07-11T22:23:07.739770Z",
     "shell.execute_reply.started": "2025-07-11T22:23:06.092113Z"
    }
   },
   "outputs": [],
   "source": [
    "from general_utils import load_config\n",
    "import torch\n",
    "\n",
    "CONFIG = load_config()\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1522f3d2-e83d-447d-9d19-7d834454333e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T22:16:35.665667Z",
     "iopub.status.busy": "2025-07-11T22:16:35.665414Z",
     "iopub.status.idle": "2025-07-11T22:16:35.768321Z",
     "shell.execute_reply": "2025-07-11T22:16:35.767834Z",
     "shell.execute_reply.started": "2025-07-11T22:16:35.665650Z"
    }
   },
   "outputs": [],
   "source": [
    "from general_utils import S3Manager\n",
    "s3_client = S3Manager.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4023a75d-14ea-4dd4-a936-338d93a65f10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T22:16:35.769396Z",
     "iopub.status.busy": "2025-07-11T22:16:35.769220Z",
     "iopub.status.idle": "2025-07-11T22:16:38.677757Z",
     "shell.execute_reply": "2025-07-11T22:16:38.677251Z",
     "shell.execute_reply.started": "2025-07-11T22:16:35.769381Z"
    }
   },
   "outputs": [],
   "source": [
    "S3Manager.download_files(\n",
    "    s3_client,\n",
    "    'model_training/data',\n",
    "     [\"training.json\",\n",
    "     \"test_queries.jsonl\",\n",
    "     \"corpus.jsonl\",\n",
    "     \"test_qrels.jsonl\"\n",
    "    ],\n",
    "    \"medical-qa-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55c781-3fd9-4bf8-982d-5bf0c9b10dc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T22:13:26.773806Z",
     "iopub.status.busy": "2025-07-11T22:13:26.773541Z",
     "iopub.status.idle": "2025-07-11T22:13:26.779107Z",
     "shell.execute_reply": "2025-07-11T22:13:26.778412Z",
     "shell.execute_reply.started": "2025-07-11T22:13:26.773788Z"
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38801a66-c244-4723-b841-8980e1aef5a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T22:16:38.678420Z",
     "iopub.status.busy": "2025-07-11T22:16:38.678266Z",
     "iopub.status.idle": "2025-07-11T22:17:09.472493Z",
     "shell.execute_reply": "2025-07-11T22:17:09.471498Z",
     "shell.execute_reply.started": "2025-07-11T22:16:38.678406Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torchrun --nproc_per_node 1 -m FlagEmbedding.finetune.embedder.encoder_only.base --model_name_or_path BAAI/bge-small-en-v1.5 --train_data model_training/data/training.json --query_instruction_for_retrieval Represent this sentence for searching relevant passages: --output_dir model_training/model --learning_rate 1e-05 --fp16 --num_train_epochs 1 --per_device_train_batch_size 20 --query_max_len 256 --passage_max_len 512 --warmup_ratio 0.05 --normalize_embeddings True --logging_steps 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 22:16:43.835107: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752272203.851190    8285 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752272203.856200    8285 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-11 22:16:43.871930: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "07/11/2025 22:16:47 - WARNING - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: True\n",
      "07/11/2025 22:16:47 - INFO - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Training/evaluation parameters AbsEmbedderTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_on_start=False,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "eval_use_gather_object=False,\n",
      "fix_position_embedding=False,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "kd_loss_type=kl_div,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=1e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=model_training/model/runs/Jul11_22-16-47_default,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "negatives_cross_device=False,\n",
      "no_cuda=False,\n",
      "normalize_embeddings=True,\n",
      "num_train_epochs=1.0,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=model_training/model,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=20,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['mlflow', 'tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=model_training/model,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=SaveStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sentence_pooling_method=cls,\n",
      "skip_memory_metrics=True,\n",
      "sub_batch_size=None,\n",
      "temperature=0.02,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "07/11/2025 22:16:47 - INFO - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Model parameters AbsEmbedderModelArguments(model_name_or_path='BAAI/bge-small-en-v1.5', config_name=None, tokenizer_name=None, cache_dir=None, trust_remote_code=False, token=None)\n",
      "07/11/2025 22:16:47 - INFO - FlagEmbedding.abc.finetune.embedder.AbsRunner -   Data parameters AbsEmbedderDataArguments(train_data=['model_training/data/training.json'], cache_path=None, train_group_size=8, query_max_len=256, passage_max_len=512, pad_to_multiple_of=None, max_example_num_per_dataset=100000000, query_instruction_for_retrieval='Represent this sentence for searching relevant passages:', query_instruction_format='{}{}', knowledge_distillation=False, passage_instruction_for_retrieval=None, passage_instruction_format='{}{}', shuffle_ratio=0.0, same_dataset_within_batch=False, small_threshold=0, drop_threshold=0)\n",
      "07/11/2025 22:16:48 - INFO - FlagEmbedding.finetune.embedder.encoder_only.base.runner -   Config: BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/11/2025 22:16:48 - INFO - FlagEmbedding.abc.finetune.embedder.AbsDataset -   loading data from model_training/data/training.json ...\n",
      "Generating train split: 11980 examples [00:00, 14242.82 examples/s]\n",
      "/opt/conda/lib/python3.12/site-packages/FlagEmbedding/finetune/embedder/encoder_only/base/runner.py:75: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `EncoderOnlyEmbedderTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = EncoderOnlyEmbedderTrainer(\n",
      "  0%|          | 0/599 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/opt/conda/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2714: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "  2%|▏         | 10/599 [00:07<07:03,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5164, 'grad_norm': 11.60119915008545, 'learning_rate': 2.3333333333333336e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 20/599 [00:14<06:50,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5128, 'grad_norm': 7.791902542114258, 'learning_rate': 5.666666666666667e-06, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 22/599 [00:16<06:49,  1.41it/s]W0711 22:17:08.925000 8283 site-packages/torch/distributed/elastic/agent/server/api.py:719] Received 2 death signal, shutting down workers\n",
      "W0711 22:17:08.926000 8283 site-packages/torch/distributed/elastic/multiprocessing/api.py:897] Sending process 8285 closing signal SIGINT\n",
      "[rank0]: Traceback (most recent call last):\n",
      "[rank0]:   File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "[rank0]:   File \"<frozen runpy>\", line 88, in _run_code\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/FlagEmbedding/finetune/embedder/encoder_only/base/__main__.py\", line 31, in <module>\n",
      "[rank0]:     main()\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/FlagEmbedding/finetune/embedder/encoder_only/base/__main__.py\", line 27, in main\n",
      "[rank0]:     runner.run()\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/FlagEmbedding/abc/finetune/embedder/AbsRunner.py\", line 149, in run\n",
      "[rank0]:     self.trainer.train(resume_from_checkpoint=self.training_args.resume_from_checkpoint)\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/transformers/trainer.py\", line 2240, in train\n",
      "[rank0]:     return inner_training_loop(\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/transformers/trainer.py\", line 2555, in _inner_training_loop\n",
      "[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/transformers/trainer.py\", line 3791, in training_step\n",
      "[rank0]:     self.accelerator.backward(loss, **kwargs)\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/accelerate/accelerator.py\", line 2469, in backward\n",
      "[rank0]:     self.scaler.scale(loss).backward(**kwargs)\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/torch/_tensor.py\", line 626, in backward\n",
      "[rank0]:     torch.autograd.backward(\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "[rank0]:     _engine_run_backward(\n",
      "[rank0]:   File \"/opt/conda/lib/python3.12/site-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n",
      "[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "[rank0]: KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(cmd))\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# actually run it\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/subprocess.py:552\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 552\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    554\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/subprocess.py:1203\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1201\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1202\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/subprocess.py:1266\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1264\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/subprocess.py:2061\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2060\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 2061\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   2063\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   2064\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   2065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/subprocess.py:2019\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2017\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   2018\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2019\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   2021\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   2024\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "lr = float(CONFIG['LR'])\n",
    "epochs = int(CONFIG['EPOCHS'])\n",
    "warmup = float(CONFIG['WARMUP_RATIO'])\n",
    "batch_size = int(CONFIG['BATCH_SIZE'])\n",
    "embedding_model = \"bge-small-en-v1.5\"\n",
    "query_instruction = CONFIG[\"QUERY_INSTRUCTION_AT_RETRIEVAL\"]\n",
    "\n",
    "cmd = [\n",
    "    \"torchrun\", \"--nproc_per_node\", \"1\",\n",
    "    \"-m\", \"FlagEmbedding.finetune.embedder.encoder_only.base\",\n",
    "    \"--model_name_or_path\", f\"BAAI/{embedding_model}\",\n",
    "    \"--train_data\", \"model_training/data/training.json\",\n",
    "    \"--query_instruction_for_retrieval\", query_instruction,\n",
    "    \"--output_dir\", \"model_training/model\",\n",
    "    \"--learning_rate\", str(lr),\n",
    "    \"--fp16\",\n",
    "    \"--num_train_epochs\", str(epochs),\n",
    "    \"--per_device_train_batch_size\", str(batch_size),\n",
    "    \"--query_max_len\", \"256\",\n",
    "    \"--passage_max_len\", \"512\",\n",
    "    \"--warmup_ratio\", str(warmup),\n",
    "    \"--normalize_embeddings\", \"True\",\n",
    "    \"--logging_steps\", \"10\",\n",
    "]\n",
    "\n",
    "# see exactly what you'll run:\n",
    "print(\" \".join(cmd))\n",
    "\n",
    "# actually run it\n",
    "result = subprocess.run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2943fbf-1c54-491f-91db-930ba9b7314d",
   "metadata": {},
   "source": [
    "Save the model files to S3 for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe2043-e0ac-42d9-ab94-d357b25883f9",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-11T22:17:09.472888Z",
     "iopub.status.idle": "2025-07-11T22:17:09.473085Z",
     "shell.execute_reply": "2025-07-11T22:17:09.472999Z",
     "shell.execute_reply.started": "2025-07-11T22:17:09.472990Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "file_list = []\n",
    "for root, dirs, files in os.walk('model_training/model/'):\n",
    "    if root=='model_training/model/':\n",
    "        for file in files:\n",
    "            file_list.append(file)\n",
    "\n",
    "S3Manager.upload_bulk(\n",
    "    s3_client,\n",
    "    'model_training/model/',\n",
    "    file_list,\n",
    "    \"medical-qa-data\",\n",
    "    \"finetuned_model/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9157f7-374a-4180-bca7-78c80c3cbbe3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d61039-325b-4a37-842d-69ccb41eb41a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-11T22:17:09.473890Z",
     "iopub.status.idle": "2025-07-11T22:17:09.474111Z",
     "shell.execute_reply": "2025-07-11T22:17:09.474014Z",
     "shell.execute_reply.started": "2025-07-11T22:17:09.474005Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "queries = load_dataset(\"json\", data_files=\"model_training/data/test_queries.jsonl\")[\"train\"]\n",
    "corpus = load_dataset(\"json\", data_files=\"model_training/data/corpus.jsonl\")[\"train\"]\n",
    "qrels = load_dataset(\"json\", data_files=\"model_training/data/test_qrels.jsonl\")[\"train\"]\n",
    "\n",
    "queries_text = queries[\"text\"]\n",
    "corpus_text = [text for text in corpus[\"text\"]]\n",
    "qrels_dict = {}\n",
    "for line in qrels:\n",
    "    if line['qid'] not in qrels_dict:\n",
    "        qrels_dict[str(line['qid'])] = {}\n",
    "    for doc in line['docid']:\n",
    "        qrels_dict[str(line['qid'])][str(doc)] = line['relevance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a11d8-a368-4efd-a4b8-7689cba335e2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-11T22:17:09.474909Z",
     "iopub.status.idle": "2025-07-11T22:17:09.475096Z",
     "shell.execute_reply": "2025-07-11T22:17:09.475013Z",
     "shell.execute_reply.started": "2025-07-11T22:17:09.475004Z"
    }
   },
   "outputs": [],
   "source": [
    "from FlagEmbedding.abc.evaluation.utils import evaluate_metrics, evaluate_mrr\n",
    "from FlagEmbedding import FlagModel\n",
    "from model_training.utils import Validator\n",
    "\n",
    "k_values = [3, 5, 10]\n",
    "\n",
    "raw_name = \"BAAI/bge-large-en-v1.5\"\n",
    "finetuned_path = \"model_training/model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5db5bc-7045-4f3a-81f7-a88120e28c9b",
   "metadata": {},
   "source": [
    "Raw model w/o prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec6ad8-bb2b-4eac-897d-584baf7234b3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-11T22:17:09.476038Z",
     "iopub.status.idle": "2025-07-11T22:17:09.476258Z",
     "shell.execute_reply": "2025-07-11T22:17:09.476168Z",
     "shell.execute_reply.started": "2025-07-11T22:17:09.476158Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_model = FlagModel(\n",
    "    raw_name, \n",
    "    query_instruction_for_retrieval=\"\",\n",
    "    devices=[0],\n",
    "    use_fp16=True\n",
    ")\n",
    "\n",
    "\n",
    "results, _ = Validator.search(raw_model, queries_text, corpus_text, queries)\n",
    "eval_res = evaluate_metrics(qrels_dict, results, k_values)\n",
    "mrr = evaluate_mrr(qrels_dict, results, k_values)\n",
    "\n",
    "for res in eval_res:\n",
    "    print(res)\n",
    "print(mrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8484e1e4-aa24-46de-8b02-a462af7395be",
   "metadata": {},
   "source": [
    "Raw model with prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c62338-c838-43a3-a562-ae205209764d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-11T22:17:09.477065Z",
     "iopub.status.idle": "2025-07-11T22:17:09.477261Z",
     "shell.execute_reply": "2025-07-11T22:17:09.477176Z",
     "shell.execute_reply.started": "2025-07-11T22:17:09.477167Z"
    }
   },
   "outputs": [],
   "source": [
    "prompted_raw_model = FlagModel(\n",
    "    raw_name, \n",
    "    query_instruction_for_retrieval=CONFIG['QUERY_INSTRUCTION_AT_RETRIEVAL'],\n",
    "    devices=[0],\n",
    "    use_fp16=True\n",
    ")\n",
    "\n",
    "\n",
    "results, _ = Validator.search(prompted_raw_model, queries_text, corpus_text, queries)\n",
    "eval_res = evaluate_metrics(qrels_dict, results, k_values)\n",
    "mrr = evaluate_mrr(qrels_dict, results, k_values)\n",
    "\n",
    "for res in eval_res:\n",
    "    print(res)\n",
    "print(mrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d21c56-6afb-4a18-9348-d3e2d1717ffd",
   "metadata": {},
   "source": [
    "Finetuned model with prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4cba10-3e08-4448-a317-eda3680afdd4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-11T22:17:09.478182Z",
     "iopub.status.idle": "2025-07-11T22:17:09.478376Z",
     "shell.execute_reply": "2025-07-11T22:17:09.478293Z",
     "shell.execute_reply.started": "2025-07-11T22:17:09.478284Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 22/599 [00:18<07:58,  1.21it/s]\n",
      "[rank0]:[W711 22:17:10.895540983 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "ft_model = FlagModel(\n",
    "    finetuned_path, \n",
    "    query_instruction_for_retrieval=CONFIG['QUERY_INSTRUCTION_AT_RETRIEVAL'],\n",
    "    devices=[0],\n",
    "    use_fp16=True\n",
    ")\n",
    "\n",
    "results, corpus_embeddings = Validator.search(ft_model, queries_text, corpus_text, queries)\n",
    "eval_res = evaluate_metrics(qrels_dict, results, k_values)\n",
    "mrr = evaluate_mrr(qrels_dict, results, k_values)\n",
    "\n",
    "for res in eval_res:\n",
    "    print(res)\n",
    "print(mrr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8ce68-a16c-4c82-a5d0-e9e247f99308",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Ingest into OpenSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1667c-e4f2-4a05-9ceb-b72e966a734b",
   "metadata": {},
   "source": [
    "Finally we will ingest our finetuned embeddings into opensearch so the can be used at inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd5e34b-5f8e-4729-a3b7-e6be4049d04b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T22:21:31.992806Z",
     "iopub.status.busy": "2025-07-11T22:21:31.992363Z",
     "iopub.status.idle": "2025-07-11T22:21:33.317627Z",
     "shell.execute_reply": "2025-07-11T22:21:33.316915Z",
     "shell.execute_reply.started": "2025-07-11T22:21:31.992788Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q opensearch_py==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44a47e2-f40f-4e5b-bc65-e6374c37b869",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T22:23:09.231137Z",
     "iopub.status.busy": "2025-07-11T22:23:09.230794Z",
     "iopub.status.idle": "2025-07-11T22:23:09.975809Z",
     "shell.execute_reply": "2025-07-11T22:23:09.975239Z",
     "shell.execute_reply.started": "2025-07-11T22:23:09.231120Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from model_training.utils import OpenSearchManager\n",
    "index_name = 'embedding-finetuned-v1'\n",
    "host = CONFIG['OPENSEARCH_INDEX_URL'].removeprefix('https://').removesuffix('/'+index_name)\n",
    "opens_mngr = OpenSearchManager(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894f55c5-d804-41a0-90b5-a8b5cd706b55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T22:23:11.483987Z",
     "iopub.status.busy": "2025-07-11T22:23:11.483552Z",
     "iopub.status.idle": "2025-07-11T22:23:11.514781Z",
     "shell.execute_reply": "2025-07-11T22:23:11.514278Z",
     "shell.execute_reply.started": "2025-07-11T22:23:11.483970Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Index already exists. Skipping creation.\n"
     ]
    }
   ],
   "source": [
    "opens_mngr.create_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f8f30-1376-412e-b7ff-aa37f54c000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opens_mngr.bulk_ingestion(index_name, corpus_text, corpus_embeddings)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
